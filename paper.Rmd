---
title: "How to use replication assignments for teaching integrity in empirical archaeology"
author:
  - Ben Marwick, bmarwick@uw.edu
  - Liying Wang
  - Ryan Robinson
  - Hope Loiselle
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
    bookdown::html_document2:
      fig_caption: yes
      reference_docx: "../templates/template.docx" # Insert path for the DOCX file
bibliography: references.bib
csl: "../templates/journal-of-archaeological-science.csl" # Insert path for the bib-style
abstract: |
  Text of abstract
keywords: |
  keyword 1; keyword 2; keyword 3
highlights: |
  These are the highlights. 
---




```{r, setup, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  comment = "#>",
  fig.path = "../figures/"
)
```


```{r setup2, eval = FALSE}
gh repo: https://github.com/benmarwick/replicationteachingarchaeology
# devtools::install_github("ekothe/rmdrive")
library(rmdrive)


# get text from google doc into Rmd
download_rmd(file = "paper",  # do not include the .Rmd 
             gfile = "Paper: Replication projects for teaching integrity in empirical archaeology")  # name of google doc file


# push updates from Rmd to google doc
update_rmd(file = "analysis/paper/paper",    # do not include the .Rmd 
           gfile = "Paper: Replication projects for teaching integrity in empirical archaeology")  # name of google doc file


```


## Introduction


In his influential study of replication, sociologist of science Harry Collins argued that replication is at the core of scientific practice, writing “Replicability [...] is the Supreme Court of the scientific system” (Collins 1985: 19). Like other observers of science, Collins claimed that in replication, private observations become communal facts, offering vital protection from error and fraud. In this paper we propose a new type of assignment for the archaeology classroom, the replication report, to better align the practice of teaching archaeology with the transparency and openness ideals of science (Nosek et al 2015). The replication report assignment involves three key steps: 1) students analysing a published report to determine what are the main claims made by the authors of that report, 2) students obtaining the data used by the authors, and 3) analysing that data to determine if one or more of the authors’ claims are reliable.  


We describe how to implement a replication report assignment suitable for upper level undergraduates and graduate students in archaeology. Our experience is based on an upper level archaeology class on stone artefact analysis taught during the Spring quarter of 2019 at the University of Washington. The class format includes a weekly cycle of lecture, discussion seminar, and hands-on laboratory activities. The assignments include seminar notes, lecture quizzes, laboratory worksheets, and two longer empirical reports. For the term that we report here, the class had 16 students, and one graduate student teaching assistant. This is a typical size for this class, and similar to the usual size of upper-level laboratory classes in the archaeology program at the University of Washington. Our students are mostly in social science and humanities majors with diverse levels of statistical knowledge. Here we survey the literature on similar types of assignments in other fields to identify common elements that other fields have identified as important principles and skills. We describe our assignment and discuss student feedback on our implementation. Finally, we offer recommendations for how to use replication reports to teach archaeology students. 


## What is replication? 


Barba (2018) points out that there is confusion and conflicting uses of the terms "reproducibility" and "replication" in recent years. In her survey of relevant literature from many disciplines, Barba finds that some fields make no distinction between "reproducibility" and "replication", and among fields that do recognise a distinction between the two, the meanings are sometimes directly swapped from one discipline to another. Here we follow what Barba has identified as the most common, long-established and highly cited definitions of these terms, and these are also the same as recently recommended by the National Academies of Sciences, Engineering, and Medicine (2019). Reproducibility is the ability to obtain results by using the same data, code, and procedures provided by the original authors (cf. Marwick 2017). This is only possible when the authors make all those materials available, for example in a research compendium (Marwick et al. 2018, Gentleman and Temple Lang 2007). Replication is the ability to arrive at the same scientific conclusions in a new study, collecting new data (possibly with different methods) and completing new analyses. 


In the following section we briefly survey replication assignments described in other fields to survey the variety of forms this usually takes. Replication assignments across different fields may not strictly fit into the above definition of replication because they do not always involve a completely new study. We consider that if a study departs from any of the original materials e.g. new data with previously published code, or new code with previously published data, then it is broadly within the definition of replication. 


## How do different disciplines use replication assignments?


Some of the earliest discussions of replication in university curricula appear in economics and psychology. Ball and Medeiros (2012) describe their TIER (teaching integrity in empirical research) protocol for undergraduate economics students at Haverford College. This is intended to ensure that a student's work is replicable by the instructors. Their protocol requires that when students submit their final project report, their submission must contain four elements: the raw data files, a metadata file, script files of code used to analyse the data, and public availability (i.e. deposit on an open repository such as Dataverse). Frank and Saxe (2012) describe how they teach undergraduates (at MIT) and graduate students (at Stanford) to do in-class replications of recent, cutting-edge psychology experiments, and note that several projects from their undergraduate course have even been part of successful publications. More recently Hawkins et al. (2018) reported on 11 replication assignments from a psychology graduate seminar at Stanford, finding that the replications typically yielded effects that were smaller than the originally published ones. Similarly, Jern (2018) describes how students in a psychology course completed replication assignments by using statistical methods of the original research articles with new data collected by the students outside of class. 


Students in Stanford University’s graduate course ‘Advanced Topics in Networking’ are given a replication assignment in which they are asked to replicating “classic” computer networking experiments (Yan and McKeown 2017). Students work in pairs and receive modest instructor support. The assignment entails selecting appropriate emulation software, communicating with the original authors, obtaining the authors’ materials, replicating the experiment, and publicising their results through both an in-class presentation and a blog post on a program website. We classify this as a replication assignment because many students could not obtain the original code from the authors, and had to write their own for the assignment. Since 2012, over 200 undergraduate and graduate students have participated in this assignment with an 86% success rate (Yan and McKeown 2017). Student feedback suggests high satisfaction with the assignment, citing unique educational value, improved understanding of the original material, and the acquisition of professional skills. In some cases, students personally contributed to the network engineering literature when their replications exposed inaccuracies in original experiments, which were then presented to and publicly amended by the authors (Yan and McKeown 2017). 


In describing her political science classes at the University of Cambridge, Janz (2016) argues that reproducibility and replication should be held as the gold standard for scientific research. She claims that teaching these concepts should be a necessary component of graduate studies, to ensure students can make their own future work reproducible. Janz reports on her class in which about 15 students undertake replication assignments over eight weeks, including providing weekly updates to each other to gain insight and feedback. Janz describes two possible levels of assignment suitable for different lengths of the term and levels of the students: duplication (aiming for the exact same results based on the exact same data set with exactly the same methods) and replication (tests the robustness of previous research results by employing newly collected data, and/or new variables, and/ or new model specifications). Duplication, which we would define here as reproduction, may be beneficial for lower-level students, while upper-level students can replicate a study and contribute original data, potentially leading to publication. Janz (2016) describes how replication assignments are a growing trend in political science departments (noting that R and STATA are commonly used), and reviews many of the practical challenges of doing replication assignments in a graduate course. She also responds to six typical criticisms of replication assignments, and points out the need for universities to nurture a culture of reproducibility and replication to ensure that the gold standard of reliable, credible, and valid research is not just an empty phrase.


The Freie Universität Berlin extracurricular graduate seminar course “Digital Open Science” aims to teach open science practice and assigns replication projects, mostly around neuroimaging topics (Toelch and Ostwald 2018). These projects are carried out with a variety of typical open science software tools and services, including Python, R, Git, GitHub and the Open Science Framework. Students first receive extensive lectures and hands-on tutoring, then choose a simple neuroscience experiment to replicate. Finally, they present their results at a symposium. The course’s primary goal is to teach students the value of verifying data upon which their own future research might rely. Students report a high rate of engaging in open science practices after taking the class, and 80% of the participants said that they believe the open science techniques will improve their future research as professionals (Toelch and Ostwald 2018). Millman et al (2018) describe a similar course at Berkeley that teaches students how to use open science tools to complete a capstone replication assignment on neuroscience topics. 


This brief survey demonstrates that replication assignments are widely known in economics, psychology, political science, and neuroscience. Common elements include group work, use of open science software and services to make the replication results openly accessible to anyone, and a scaffolded, stepwise approach to the task to ensure that students receive instructor support at multiple stages in the assignment. To the best of our knowledge replication assignments are not common in archaeology programs. We posted a message to the Society of American Archaeology Teaching Archaeology Interest Group e-community to ask for examples of replication assignments used in teaching archaeology, and received no replies from anyone teaching with replication. 


## How to conduct a replication assignment in archaeology


In this section we describe our replication assignment and how we assessed its effectiveness. A brief discussion of our replication report assignment was announced at the beginning of the ten week-long term to give students background about the purpose and concepts of replication and our expectations. Our replication report assignment consisted of three small, graded activities to scaffold the preparation of the final report. The first step started from Week Four and each step was separated by one week to give students time to work and submit their final reports due on Week Seven. Students were expected to work in groups of 3-4 people, but submit their assignments for each of the three steps and the final report individually. Submissions for each step were graded as complete/incomplete, with feedback provided individually via the Canvas learning management system, and collectively during class meetings. Our course had no prerequisites, so we assumed no prior knowledge of the R programming language among the students, and were prepared to teach them as complete novices. We chose R (R Core Team 2019) because it is widely used by archaeologists (Schimdt and Marwick 2019), and also commonly taught in undergraduate classes in social sciences and statistics (Baumer et al. 2014; Çetinkaya-Rundel and Rundel 2018; Dvorak et al 2019). We were also prepared for students to have no prior experience with replication assignments.


### Step 1: In groups, select a study to replicate


For the first step we supplied students with a list of journal articles that included raw data and R code either in supplemental files or deposited on open data repositories. This list is updated regularly, but is not exhaustive, and is currently online at https://github.com/benmarwick/ctv-archaeology. Working in groups of 3-4 people, students selected a journal article from this list as their target article for this assignment. We encouraged them to choose a target article about a stone artefact analysis that looked interesting to them. We also required students to set up an open communication channel for their group to ensure they had an easy way to discuss their selection of the target article. We used Slack (https://slack.com/, Perkel 2016), a free cloud-based web application for team communication, to help them collaborate with each other efficiently. The instructors were members of all the student group channels to supervise, give guidance and support good communication habits. Students were required to individually submit the full bibliographic reference for their target article to complete step one.


### Step 2: Identify the key claims and data in the study


For step two of the assignment, students were required to discuss in their groups to identify 2-3 key claims made by the authors of their target article. They were told to study the data visualisations in the paper to identify which figures seem to give the best support to the authors' claims. Recreating these 1-2 visualisations was a key task for the students in the production of their final report. A second task for step two was for students to identify and obtain the raw data files of their target article. The list of articles that the students chose from only included articles for which data were openly available. This removed the need for students to contact authors to request data, which may have added the risk of a long wait for a favourable reply, refusal to share, or no reply. To complete step two, each student was required to submit a short statement summarising the 2-3 key claims of their target paper, and the raw data file. 


### Step 3: Begin the replication analysis and get instructor feedback


Step three of the assignment required students to create a file structure on their computer to organise their assignment files, following basic guidance in Marwick et al. (2017). They also had to download an R Markdown file and write a small amount of R code to read in the raw data and explore it with one basic visualisation, using data in the target article. R Markdown is a file format for making reproducible documents with R. An R Markdown document is written in markdown (a simple plain text format) and contains chunks of embedded R code (Xie et Al. 2018). The document can be easily converted into many standard formats, such as Microsoft Word, PDF and HTML. We prepared an R Markdown template file with some basic headings (following the IMRaD or  Introduction-Method-Results-and-Discussion format) and empty code chunks to provide guidance to the students on how many code chunks were expected, and where in the document they should appear. As students wrote their R code and encountered errors, they were encouraged to share screenshots on Slack for the instructors to assist with troubleshooting. After completing this step, the instructor met with each group to review the main claims identified by the students, review the visualisation they had chosen to replicate, and provide guidance on writing the R code to produce the key visualisations.  


### Step 4: Complete the replication analysis and submit the compendium of report, code and data


The final task was for the students to write their report, and submit a reproducible research compendium. This included three files: (1) their R Markdown document, (2)  the raw data file, and (3) the output document (e.g. Microsoft Word document that is produced when they knit the R Markdown file). Our expectation was that we could reproduce any student’s results by running their submitted R Markdown document with the raw data file to produce the Word document they submitted. The final report was graded with a rubric which was presented to the students at the first step to help set expectations about what the final product should look like.


In the time between students submitting their final report and the grades being released we administered an online survey on Canvas to obtain feedback anonymously from the students. The purpose of the feedback survey was to collect information about how to improve the assignment for future classes, to understand the students’ experience of the assignment, and what value they perceive in replication skills for archaeology in general, and for them individually.  Two questions were designed to learn about students' prior experiences of replication assignment and using the R language. We asked about students’ opinions and attitudes toward replication assignments in archaeology and collected responses on a likert scale. Two open-ended questions sought to know more about the students’ thoughts on replication in the classroom in general. They had one week to respond to the survey, which was not a requirement. 


## Observations on the assignment process


The first step, choosing the target article, revealed the need for some intervention from the instructor to guide students to articles that used relatively simple statistical methods. For example, one group initially chose @Breslawski_Etter_Jorgeson_Boulanger_2018 as their target article, but the key claims in this paper depend on multiple comparisons of multilevel regression models. We explained to the students that if they attempted to replicate a key claim of this paper then they would likely be doing substantially more work than other groups in the class. We invited this group to choose a different target article to ensure a more comparable experience, which they accepted. The statistical backgrounds of our students was highly diverse, so we could not expect students to be very discerning about the statistical complexity of the methods in the articles on the list of potential target articles. As a consequence, we were prepared to intervene to guide their selection of a paper that we could be sure they could successfully replicate, given the time available. 


The second step was mostly straightforward, with students engaging in discussion in class and on Slack to identify the 2-3 key claims of their target paper, and identify the data visualisation that provided the most relevant support to one or more of those claims. Given that not all students have statistical background, the instructor covered some statistical methods they might encounter while reading during lecture, such as principal components analysis, to give them the mathematical concepts behind it. Identifying the data files was less straightforward, with about one third of students failing to correctly identify the data files accompanying their target article. We attribute this to the relatively low level of familiarity of the students in working with raw data, so they are not sure when they are looking at it, and the high degree of variability in how the target article authors make their data open. Some authors include their data as a file in the supplementary information attached to the article, while others deposit their files on an open data repository such as osf.io or figshare.com, and then cite the DOI to the files in their article. Where the data files were nested in several layers of folders, some students struggled to find them. 


The ability to easily share screenshots on Slack was important to the success of the third step. Our intention was that two lab classes earlier in the term that introduced students to some methods for data visualisation using R would provide the foundation for succeeding in this step. We have expected that two lab reports completed earlier in the term that required to be written with R markdown would help them practice crucial code they might need later. For the lab reports, students used R markdown templates we provided to complete tasks of reading data into R, basic data tidying, and data visualisation by modifying sample code. However, we found that for some students this was not sufficient practice, and substantial instructor guidance was required to help them complete this step. At the completion of this step, the instructor met one on one with each group to check how successful they were producing a basic visualisation using data from the target article, and to discuss the group's strategy to complete the report. This was the most time-consuming aspect of the assignment for the instructor, holding a one hour meeting with each of the five groups. 


## Analysis of the students' anonymous feedback


Thirteen out of sixteen students completed an anonymous feedback survey (Figure X). Only one student had done replication before and two had used R previously for an archaeology assignment. Most students strongly agreed with the statements about having sufficient support and clear instructions. Most students strongly disagreed with the statement, “I am likely to attempt to replicate published research in my future studies and work”. This contrasts with the high proportion of students that agree with the statement “The ability to replicate published research is an important skill for professional archaeologists”. Taken together, these two responses show that while students see the value of replication to archaeology in general, they do not see any specific benefits to doing it themselves. This may reflect a failure of the instructor to communicate the individual benefits of developing skills for replication. It may also reflect uncertainty among the students about their plans for a career in archaeology. Most, but not all, students agreed that the replication assignment helped them to learn more effectively than reading a paper to write a traditional paper.


Figure X shows the correlations between the five feedback questions that have likert scale responses. The statements about instructions and instructor support are highly positively correlated, showing the positive effects of the assignment design, a detailed rubric, and the instructor meeting with each group to discuss their work, and answering students’ questions promptly on Slack. The strongest negative correlation is between the statements about instructor support and doing replication in future work. This might suggest that the students received so much support that they do not feel capable to do a replication like this by themselves. We see confirmation of this in the free-form comments, such as ‘it would not have been possible for us to do this correctly on our own’. These correlations indicate a need to equip students with skills to work more independently of the instructor, and strengthen students’ self-efficacy with replication skills.


## Analysis of the students' grades for the replication assignment


We graded the students’ final submissions using a rubric with criteria that covered content, the introduction, methods, result, and conclusion sections, and style. In Figure \@ref(fig:rubric-scores) we show the distribution and means of student scores for each criterion. The two criteria showing the highest mean score are “Style: use commas and apostrophes correctly, and spell consistently”, and “Intro: has clear statement of the purpose of the report”. High scores for the grammar criterion are expected because these reflect basic writing skills required for many undergraduate-level courses. Students are expected to have learned these in lower level classes before taking this class. The high scores relating to the introduction section may reflect the effectiveness of the scaffolding steps that focused students on the specific purpose of the assignment. The lowest mean score is for “Content: minimum of 4 scholarly items in the reference list” that shows some people did not include four items. This might result from insufficient prior training in searching for scholarly publications, suggesting that although this is also a skill that should have been acquired before taking this class, many students remain weak at this task. A low mean is also evident for "Intro: has names, locations, and basic chronology of sites", because some students neglected to supply these archaeological details. Future use of this assignment will incorporate these low-scoring criteria into the scaffolding steps to emphasize their importance to students and give an opportunity for early feedback to students. 


The criteria most relevant to the replication component of the assignment, in order from highest mean score to lowest, are "Content: submission includes Rmd file, Data file, and Word file",  "Conclusion: state whether the author’s claims appear to be robust, unreliable, etc", "Results: includes 1-2 original plots & description of these", and "Methods: identify the specific results you will replicate". This suggests that we could help students to develop a better skills in narrating their process (writing about methods), and describing and interpreting their data visualizations. In the future we may include more fundamental exercises focusing on these tasks in the scaffolding steps. Overall, we find that comparison of the scores for the replication criteria and other criteria shows there is no clear evidence that the replication component of this assignment lowers students’ grades. The two lowest scoring criteria are more generic research and writing skills rather than skills specific to the replication aspect of the assignment. 


## Discussion 


Replication of results is widely claimed as a gold standard in science. When a result can be independently validated, we can build on it to advance knowledge in our field. Teaching students about replication and giving them skills to conduct replication is thus a vital part of preparing them for professional work in scientific archaeology. The immediate practical benefits of students doing replication assignments include realistic experience with analysing and visualising real-word data (rather than toy datasets often used for class activities), and getting them working at the research frontier by taking an in-depth look at recently published work, going beyond the usual reading and discussion.


The longer term practical benefits include cultivating a reproducibility routine for students to develop a natural habit of organising their code and data for future work so another person can use it to reproduce their results. Benefits also include developing professionalism among students: by working through the steps of an analysis students gain an understanding of what kinds of decisions in all steps of an analysis are acceptable (Janz 2018). Although the small scale of our assignment did not offer the potential for students to publish new findings from replication, we anticipate this may be a benefit for archaeology students participating in more extensive replication assignments.


The challenges of requiring students to do replication assignments are similar to those faced in many types of archaeology classes with quantitative skills at their core. In our case, the absence of a prerequisite and the high variability of statistical and programming skills meant that some students needed a lot more support than others. This may make replication assignments impractical where instructor-student contact hours are limited. In addition to time, the instructor needs to have a high skill level in quantitative methods to guide students in their engagement with the literature. The instructor will also benefit from a high tolerance for helping students to solve coding problems. To mitigate this in the future we will add a step of student peer review to give students an opportunity to get feedback and assistance from each other more formally.


## Conclusion


Our main finding is that replication assignments are valued and in common use throughout the social sciences, and that they can be effective in teaching archaeology. Specifically, we found it possible to conduct a small scale replication assignment as part of an upper level undergraduate class. Student feedback was positive


What are our main findings?


What are the limitations of our study? Small sample size might affect statistical power, feedback survey as an assessment method might be subjective (comparison with grade ), control conditions: other factors that might have an impact on the replication paper


What recommendations can we make to other archaeology instructors?
Learning motivation, understanding of statistics and principles behind graphs with some exercises  


What will we do differently in the future?


## Acknowledgements






<!-- The following line inserts a page break when the output is MS Word. For page breaks in PDF, use \newpage on its own line.  -->
##### pagebreak


# References 
<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->
<div id="refs"></div>


Nosek, B. A., Alter, G., Banks, G. C., Borsboom, D., Bowman, S. D., Breckler, S. J., ... & Contestabile, M. (2015). Promoting an open research culture. Science, 348(6242), 1422-1425.


Cliangirig Order: Replication arid Induction in Scientific Practice. BY H. M.
COLLINS. London, Beverly Hills, and New Delhi: Sage Publications, 1985.
Pp. viii -+ 187.


Marwick, B. (2017). Computational reproducibility in archaeological research: basic principles and a case study of their implementation. Journal of Archaeological Method and Theory, 24(2), 424-450.


Xie, Y., Allaire, J. J., & Grolemund, G. (2018). R markdown: The definitive guide. CRC Press.




@Manual{,
  title        = {R: A Language and Environment for Statistical
                  Computing},
  author       = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address      = {Vienna, Austria},
  year         = YEAR,
  url          = {https://www.R-project.org}
}
where YEAR is




##### pagebreak


### Colophon


This report was generated on `r Sys.time()` using the following computational environment and dependencies: 


```{r colophon, cache = FALSE}
# which R packages and versions?
devtools::session_info()
```


The current Git commit details are:


```{r}
# what commit is this file at? 
git2r::repository(here::here())
```